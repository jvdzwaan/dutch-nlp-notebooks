Models
======

List of pretrained word embeddings and other models.

Word embeddings
---------------

* `fastText <https://fasttext.cc/docs/en/crawl-vectors.html>`_: fastText word vectors trained on Common Crawl and Wikipedia.
* `Word2Vec <https://github.com/clips/dutchembeddings>`_: Word2Vec vectors trained by CLIPS on diffent Dutch corpora.
* `Word2Vec <http://vectors.nlpl.eu/repository/>`_: Word2Vec vectors trained by the Nordic Language Processing Laboratory (NLPL) on the CoNLL17 corpus.

ULMFiT
------

* `Leiden fastai ULMFiT model <http://textdata.nl>`_: Trained on Wikipedia by the Text Mining and Retrieval research group at Leiden University.

BERT
----

* `BERTje <https://github.com/wietsedv/bertje>`_: Dutch pre-trained BERT model developed at the University of Groningen.
* `RobBERT <https://people.cs.kuleuven.be/~pieter.delobelle/robbert/>`_: Dutch RoBERTa-based Language Model.
* `BERT-NL <http://textdata.nl>`_: cased and uncased BERT model trained on the SoNaR corpus by the Text Mining and Retrieval research group at Leiden University.
* `mBERT <https://github.com/google-research/bert/blob/master/multilingual.md>`_: Multilingual BERT model.
